{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.ensemble import IsolationForest, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import corex\n",
    "import mca\n",
    "from sklearn.linear_model import HuberRegressor, BayesianRidge, Lasso\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intialising Lists of features\n",
    "bin_features = None \n",
    "cat_features = None\n",
    "paired_bin_features = None\n",
    "paired_cat_features = None\n",
    "trans_cat_features = None\n",
    "trans_paired_bin_features = None\n",
    "trans_paired_cat_features = None\n",
    "mca_features = None\n",
    "pca_all_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Coding categerical features with the values of the target variable\n",
    "from math import exp\n",
    "\n",
    "def transform_categerical_feature(train, f, target_mean):\n",
    "    _lambda = lambda n: 1.0 / (1.0 + exp(-(n - 5.0) / 1))\n",
    "    \n",
    "    trans_df = train.groupby(f, as_index=False)['y'].agg({'var_mean': 'mean',\n",
    "                                                       'cnt': 'count'})\n",
    "    trans_df.loc[:, 'trans_' + f] = trans_df.apply(lambda r: (_lambda(r['cnt']) * r['var_mean']) +\n",
    "                                                              ((1 - _lambda(r['cnt'])) * target_mean), axis=1)\n",
    "    \n",
    "    target_mean = np.mean(train.y.values)\n",
    "    target_std = np.std(train.y.values)\n",
    "    \n",
    "    trans_df = trans_df[[f, 'trans_' + f]]\n",
    "    \n",
    "    trans_df.loc[-1] = [-1, target_mean]\n",
    "    trans_df.index = trans_df.index + 1\n",
    "    \n",
    "    trans_df.loc[-1] = [-2, target_std]\n",
    "    trans_df.index = trans_df.index + 1\n",
    "    \n",
    "    return trans_df\n",
    "\n",
    "\n",
    "def target_coding_column(data, trans_df, field):\n",
    "    data = data.merge(trans_df, on=field, how='left')    \n",
    "   \n",
    "    target_mean = trans_df.loc[(trans_df[field] == -1), 'trans_' + field].values[0]\n",
    "    target_std = trans_df.loc[trans_df[field] == -2, 'trans_' + field].values[0]\n",
    "    \n",
    "    data.loc[np.isnan(data['trans_' + field]), 'trans_' + field] = \\\n",
    "                                        np.random.normal(loc=target_mean, scale=target_std\n",
    "                                        , size=(len(data.loc[np.isnan(data['trans_' + field])])))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def target_coding_training(train_df, field):\n",
    "    kfold = KFold(n_splits=5)\n",
    "    tmp = pd.DataFrame()\n",
    "\n",
    "    for itrain, itest in kfold.split(np.zeros(len(train_df))):\n",
    "        target_mean = np.mean(train_df.iloc[itrain].y.values)\n",
    "\n",
    "        trans_df = transform_categerical_feature(train_df.iloc[itrain], field, target_mean)\n",
    "        tmp = tmp.append(target_coding_column(train_df.iloc[itest], trans_df, field))\n",
    "\n",
    "    target_mean = np.mean(train_df.y.values)\n",
    "    trans_df = transform_categerical_feature(train_df, field, target_mean)\n",
    "    return tmp, trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./train.csv.zip', compression='zip')\n",
    "test_data = pd.read_csv('./test.csv.zip', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_features = list(data.columns)        \n",
    "all_features.remove('y')\n",
    "all_features.remove('ID')\n",
    "\n",
    "cat_features = ['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8'] # Categerical features\n",
    "bin_features = [ x for x in all_features if x not in cat_features] # Binary featuers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([data, test_data])\n",
    "\n",
    "# Coding categerical featuers\n",
    "for c in cat_features:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values))\n",
    "    data.loc[:, c] = lbl.transform(list(data[c].values))\n",
    "    test_data.loc[:, c] = lbl.transform(list(test_data[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data['y'] = -1\n",
    "all_data = pd.concat([data, test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Identical features that will be disregarded\n",
    "\n",
    "identical_features = ['X214', 'X227', 'X360', 'X172', 'X364', 'X253', 'X84', 'X213',\n",
    "                      'X382', 'X216', 'X293', 'X119', 'X302', 'X279', 'X296', 'X134', \n",
    "                      'X324', 'X326', 'X39', 'X37', 'X35', 'X76', 'X199', 'X299', 'X113', \n",
    "                      'X222', 'X226', 'X102', 'X385', 'X254', 'X147', 'X146', 'X330', \n",
    "                      'X262', 'X239', 'X244']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the indentical features\n",
    "identical_features = list(set(identical_features))\n",
    "bin_features = [x for x in bin_features if x not in identical_features]\n",
    "all_features = [x for x in all_features if x not in identical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paired X11 with: X74 X205\n",
      "Paired X15 with: X74 X205\n",
      "Paired X29 with: X136 X263\n",
      "Paired X33 with: X74 X205\n",
      "Paired X40 with: X205\n",
      "Paired X42 with: X205\n",
      "Paired X52 with: X120\n",
      "Paired X54 with: X136 X263\n",
      "Paired X59 with: X205\n",
      "Paired X74 with: X93 X95 X107 X124 X190 X204 X210 X233 X235 X236 X252 X257 X259 X260 X268 X270 X278 X280 X288 X289 X290 X295 X297 X319 X339 X347 X369 X372\n",
      "Paired X83 with: X205\n",
      "Paired X86 with: X205\n",
      "Paired X87 with: X205\n",
      "Paired X89 with: X205\n",
      "Paired X93 with: X205\n",
      "Paired X95 with: X205\n",
      "Paired X105 with: X205\n",
      "Paired X107 with: X205\n",
      "Paired X110 with: X205\n",
      "Paired X124 with: X205\n",
      "Paired X128 with: X130\n",
      "Paired X136 with: X232\n",
      "Paired X142 with: X158\n",
      "Paired X145 with: X205\n",
      "Paired X153 with: X205\n",
      "Paired X156 with: X157\n",
      "Paired X160 with: X205\n",
      "Paired X167 with: X205\n",
      "Paired X184 with: X205\n",
      "Paired X186 with: X194\n",
      "Paired X190 with: X205\n",
      "Paired X204 with: X205\n",
      "Paired X205 with: X207 X210 X233 X235 X236 X245 X252 X257 X258 X259 X260 X266 X268 X269 X270 X277 X278 X280 X288 X289 X290 X295 X297 X318 X319 X332 X339 X347 X366 X369 X372 X383 X384\n",
      "Paired X228 with: X229\n",
      "Paired X232 with: X263\n"
     ]
    }
   ],
   "source": [
    "# Pairing binary features that are more than 99.8% different \n",
    "first = True\n",
    "n = float(len(all_data))\n",
    "paired_bin_features = []\n",
    "bin_f = bin_features\n",
    "for f1 in bin_features[:-1]:\n",
    "    first = True\n",
    "    bin_f = bin_f[1:]\n",
    "    for f2 in bin_f:\n",
    "        diff = len(all_data.loc[all_data[f1] != all_data[f2]]) / n\n",
    "        if diff > 0.998:\n",
    "            if first:\n",
    "                print\n",
    "                print 'Paired %s with: %s'%  (f1, f2),\n",
    "                first = False\n",
    "            else:\n",
    "                print f2,\n",
    "            ff1 = 'X'+str(max([int(f1[1:]), int(f2[1:])]))\n",
    "            ff2 = 'X'+str(min([int(f1[1:]), int(f2[1:])]))\n",
    "            paired_bin_features.append(ff1 + '_' + ff2)\n",
    "            data[ff1+ '_' + ff2] = data[ff1].astype('str') + data[ff2].astype('str')\n",
    "            test_data[ff1+ '_' + ff2] = test_data[ff1].astype('str') + test_data[ff2].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_bin_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# Pairing all combinations of categerical features\n",
    "paired_cat_features = []\n",
    "cat_f = cat_features\n",
    "for f1 in cat_features[:-1]:\n",
    "    cat_f = cat_f[1:]\n",
    "    for f2 in cat_f:\n",
    "        data[f1+'_'+f2] = data[f1].astype('str') + data[f2].astype('str')\n",
    "        test_data[f1+ '_' + f2] = test_data[f1].astype('str') + test_data[f2].astype('str')\n",
    "        paired_cat_features.append(f1+ '_' + f2)\n",
    "print len(paired_cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHE: 211\n"
     ]
    }
   ],
   "source": [
    "# Code categerical feature using one-hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OHE = OneHotEncoder(sparse=False,dtype=np.int8)\n",
    "ohe_results = OHE.fit_transform(all_data[cat_features].values)\n",
    "\n",
    "cat_bin_features = []\n",
    "\n",
    "results = ohe_results[0:len(data), :]\n",
    "\n",
    "print 'OHE:', results.shape[1]\n",
    "\n",
    "for i in range(results.shape[1]):\n",
    "    data['ohe_' + str(i)] = results[:, i]\n",
    "    cat_bin_features.append('ohe_' + str(i))\n",
    "\n",
    "\n",
    "results = ohe_results[len(data):, :]\n",
    "for i in range(results.shape[1]):\n",
    "    test_data['ohe_' + str(i)] = results[:, i]\n",
    "    \n",
    "\n",
    "all_data = pd.concat([data, test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 8418\n",
      "[t-SNE] Computed conditional probabilities for sample 8418 / 8418\n",
      "[t-SNE] Mean sigma: 1.249424\n",
      "[t-SNE] KL divergence after 100 iterations with early exaggeration: 1.511334\n",
      "[t-SNE] Error after 225 iterations: 1.511334\n"
     ]
    }
   ],
   "source": [
    "# Calculate t-distributed Stochastic Neighbor Embedding for all the binary features\n",
    "from sklearn.manifold import TSNE\n",
    "tsne_feat = 5\n",
    "tsne = TSNE(n_components=tsne_feat, learning_rate=500, verbose=1)\n",
    "tsne_results = tsne.fit_transform(all_data[bin_features + cat_bin_features].values)\n",
    "\n",
    "tsne_features = []\n",
    "\n",
    "results = tsne_results[0:len(data), :]\n",
    "for i in range(tsne_feat):\n",
    "    data['tsne_' + str(i)] = results[:, i]\n",
    "    tsne_features.append('tsne_' + str(i))\n",
    "\n",
    "results = tsne_results[len(data):, :]\n",
    "for i in range(tsne_feat):\n",
    "    test_data['tsne_' + str(i)] = results[:, i]\n",
    "\n",
    "all_data = pd.concat([data, test_data])\n",
    "\n",
    "# Use to k-means to clsuter the embedding vectors to create a new categerical feature\n",
    "from sklearn.cluster import KMeans\n",
    "c = KMeans(n_clusters=100)\n",
    "c = c.fit(all_data[tsne_features].values)\n",
    "\n",
    "data['tsne_cls'] = c.predict(data[tsne_features].values)\n",
    "test_data['tsne_cls'] = c.predict(test_data[tsne_features].values)\n",
    "cat_features.append('tsne_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Isometric Mapping to calculate embedingof all the binary variables\n",
    "from sklearn.manifold import Isomap\n",
    "isomap_feat = 5\n",
    "isomap = Isomap(n_neighbors=5, n_components=isomap_feat)\n",
    "results = isomap.fit_transform(data[bin_features + cat_bin_features].values)\n",
    "\n",
    "isomap_features = []\n",
    "\n",
    "for i in range(isomap_feat):\n",
    "    data['isomap_' + str(i)] = results[:, i]\n",
    "    isomap_features.append('isomap_' + str(i))\n",
    "\n",
    "results = isomap.transform(test_data[bin_features + cat_bin_features].values)\n",
    "for i in range(isomap_feat):\n",
    "    test_data['isomap_' + str(i)] = results[:, i]\n",
    "\n",
    "\n",
    "all_data = pd.concat([data, test_data])\n",
    "\n",
    "# Use to k-means to clsuter the embedding vectors to create a new categerical feature\n",
    "from sklearn.cluster import KMeans\n",
    "c = KMeans(n_clusters=100)\n",
    "c = c.fit(all_data[isomap_features].values)\n",
    "\n",
    "data['isomap_cls'] = c.predict(data[isomap_features].values)\n",
    "test_data['isomap_cls'] = c.predict(test_data[isomap_features].values)\n",
    "cat_features.append('isomap_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Transform all the categerical and paired features by encoding them using the target variable\n",
    "\n",
    "trans_features = []\n",
    "\n",
    "for c in cat_features + paired_bin_features + paired_cat_features:\n",
    "    data, transform_df = target_coding_training(data, c)\n",
    "    test_data = target_coding_column(test_data, transform_df, c)\n",
    "    trans_features.append('trans_' + c)\n",
    "        \n",
    "data.index = range(len(data))\n",
    "test_data.index = range(len(test_data))\n",
    "\n",
    "trans_cat_features = ['trans_'+x for x in cat_features]    \n",
    "trans_paired_bin_features = ['trans_'+x for x in paired_bin_features]\n",
    "trans_paired_cat_features = ['trans_'+x for x in paired_cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data['y'] = -1\n",
    "all_data = pd.concat([data, test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA ...\n"
     ]
    }
   ],
   "source": [
    "# Calculate Linear dimensionality reduction using Singular Value Decomposition f\n",
    "# or binary and transformed features\n",
    "\n",
    "n_feat = 12\n",
    "print 'PCA ...'\n",
    "\n",
    "pca = PCA(n_components=n_feat)\n",
    "results = pca.fit_transform(data[bin_features + trans_features].values)\n",
    "\n",
    "pca_all_features = []\n",
    "\n",
    "for i in range(n_feat):\n",
    "    data['pca_' + str(i)] = results[:, i]\n",
    "    pca_all_features.append('pca_' + str(i))\n",
    "\n",
    "\n",
    "results = pca.transform(test_data[bin_features + trans_features].values)\n",
    "for i in range(n_feat):\n",
    "    test_data['pca_' + str(i)] = results[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA bin...\n"
     ]
    }
   ],
   "source": [
    "# Calculate Linear dimensionality reduction using Singular Value Decomposition f\n",
    "# or all the binary features\n",
    "\n",
    "print 'PCA bin...'\n",
    "\n",
    "pca = PCA(n_components=n_feat)\n",
    "results = pca.fit_transform(data[bin_features + cat_bin_features].values)\n",
    "\n",
    "pca_bin_features = []\n",
    "\n",
    "for i in range(n_feat):\n",
    "    data['pca_bin_' + str(i)] = results[:, i]\n",
    "    pca_bin_features.append('pca_bin_' + str(i))\n",
    "\n",
    "\n",
    "results = pca.transform(test_data[bin_features + cat_bin_features].values)\n",
    "for i in range(n_feat):\n",
    "    test_data['pca_bin_' + str(i)] = results[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRP...\n"
     ]
    }
   ],
   "source": [
    "# Use Gaussian Random Projection to reduce the dimensionality\n",
    "print 'GRP...'\n",
    "from sklearn import random_projection\n",
    "\n",
    "grp_model = random_projection.GaussianRandomProjection(n_components=n_feat)\n",
    "\n",
    "results = grp_model.fit_transform(data[bin_features + trans_features].values)\n",
    "\n",
    "grp_features = []\n",
    "\n",
    "for i in range(n_feat):\n",
    "    data['grp_' + str(i)] = results[:, i]\n",
    "    grp_features.append('grp_' + str(i))\n",
    "\n",
    "results = grp_model.transform(test_data[bin_features + trans_features].values)\n",
    "for i in range(n_feat):\n",
    "    test_data['grp_' + str(i)] = results[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCA...\n"
     ]
    }
   ],
   "source": [
    "# Multiple Correspondence Analysis to capture any interaction between all categerical and binary features\n",
    "\n",
    "print 'MCA...'\n",
    "mca_model = mca.mca(all_data[all_features], cols=all_features)\n",
    "mca_results = mca_model.fs_r(N=n_feat)\n",
    "\n",
    "mca_features = []\n",
    "\n",
    "results = mca_results[0:len(data), :]\n",
    "\n",
    "for i in range(n_feat):\n",
    "    data['mca_' + str(i)] = results[:, i]\n",
    "    mca_features.append('mca_' + str(i))\n",
    "\n",
    "\n",
    "results = mca_results[len(data):, :]\n",
    "for i in range(n_feat):\n",
    "    test_data['mca_' + str(i)] = results[:, i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCA bin...\n"
     ]
    }
   ],
   "source": [
    "# Multiple Correspondence Analysis to capture any interaction between all the binary features\n",
    "print 'MCA bin...'\n",
    "mca_model1 = mca.mca(all_data[bin_features + cat_bin_features], cols=bin_features + cat_bin_features)\n",
    "mca_results = mca_model1.fs_r(N=n_feat)\n",
    "\n",
    "mca_bin_features = []\n",
    "\n",
    "results = mca_results[0:len(data), :]\n",
    "\n",
    "for i in range(n_feat):\n",
    "    data['mca_bin_' + str(i)] = results[:, i]\n",
    "    mca_bin_features.append('mca_bin_' + str(i))\n",
    "\n",
    "results = mca_results[len(data):, :]\n",
    "for i in range(n_feat):\n",
    "    test_data['mca_bin_' + str(i)] = results[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mca_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>mca_bin_2</th>\n",
       "      <th>mca_bin_3</th>\n",
       "      <th>mca_bin_4</th>\n",
       "      <th>mca_bin_5</th>\n",
       "      <th>mca_bin_6</th>\n",
       "      <th>mca_bin_7</th>\n",
       "      <th>mca_bin_8</th>\n",
       "      <th>mca_bin_9</th>\n",
       "      <th>mca_bin_10</th>\n",
       "      <th>mca_bin_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>130.81</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007585</td>\n",
       "      <td>-0.006389</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.009172</td>\n",
       "      <td>0.064545</td>\n",
       "      <td>0.023398</td>\n",
       "      <td>-0.046245</td>\n",
       "      <td>-0.021299</td>\n",
       "      <td>0.115203</td>\n",
       "      <td>0.102771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>88.53</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006534</td>\n",
       "      <td>-0.013715</td>\n",
       "      <td>0.012769</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.008860</td>\n",
       "      <td>0.006770</td>\n",
       "      <td>-0.021232</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.014699</td>\n",
       "      <td>0.026331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>76.26</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086259</td>\n",
       "      <td>0.027286</td>\n",
       "      <td>-0.006102</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>-0.004477</td>\n",
       "      <td>-0.019870</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>0.009331</td>\n",
       "      <td>-0.003302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>80.62</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096478</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>-0.020390</td>\n",
       "      <td>-0.004860</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>-0.004164</td>\n",
       "      <td>-0.010170</td>\n",
       "      <td>-0.008580</td>\n",
       "      <td>-0.020058</td>\n",
       "      <td>-0.001327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>78.02</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095224</td>\n",
       "      <td>0.008963</td>\n",
       "      <td>-0.017589</td>\n",
       "      <td>-0.009284</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>-0.001328</td>\n",
       "      <td>-0.005175</td>\n",
       "      <td>-0.012752</td>\n",
       "      <td>-0.026793</td>\n",
       "      <td>-0.000678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 925 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       y  X0  X1  X2  X3  X4  X5  X6  X8     ...      mca_bin_2  \\\n",
       "0   0  130.81  37  23  20   0   3  27   9  14     ...       0.007585   \n",
       "1   6   88.53  37  21  22   4   3  31  11  14     ...      -0.006534   \n",
       "2   7   76.26  24  24  38   2   3  30   9  23     ...       0.086259   \n",
       "3   9   80.62  24  21  38   5   3  30  11   4     ...       0.096478   \n",
       "4  13   78.02  24  23  38   5   3  14   3  13     ...       0.095224   \n",
       "\n",
       "   mca_bin_3  mca_bin_4  mca_bin_5  mca_bin_6  mca_bin_7  mca_bin_8  \\\n",
       "0  -0.006389   0.007831   0.009172   0.064545   0.023398  -0.046245   \n",
       "1  -0.013715   0.012769   0.007381   0.008860   0.006770  -0.021232   \n",
       "2   0.027286  -0.006102   0.010600  -0.004477  -0.019870   0.002363   \n",
       "3   0.009374  -0.020390  -0.004860   0.005778  -0.004164  -0.010170   \n",
       "4   0.008963  -0.017589  -0.009284   0.000874  -0.001328  -0.005175   \n",
       "\n",
       "   mca_bin_9  mca_bin_10  mca_bin_11  \n",
       "0  -0.021299    0.115203    0.102771  \n",
       "1   0.001643    0.014699    0.026331  \n",
       "2   0.009309    0.009331   -0.003302  \n",
       "3  -0.008580   -0.020058   -0.001327  \n",
       "4  -0.012752   -0.026793   -0.000678  \n",
       "\n",
       "[5 rows x 925 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_features_list()\n",
    "\n",
    "test_data.to_csv('./test_prepared.csv', index=False)\n",
    "data.to_csv('./data_prepared.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stacked machine learning model that uses different algorithms and \n",
    "# different set of features for each algorithem\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "class stacked_model:\n",
    "    def _create_models(self, name_list):        \n",
    "        self.models = {}\n",
    "        \n",
    "        for n in name_list:\n",
    "            if 'xgb' in n:\n",
    "                self.models[n] = XGBRegressor(reg_alpha=0.5, reg_lambda=4, gamma=0.5,\n",
    "                                 learning_rate=0.005, n_estimators=1000, subsample=0.95, colsample_bytree=0.5, \n",
    "                                 objective='reg:linear', max_depth=4)\n",
    "            elif 'RF' in n:\n",
    "                self.models[n] = RandomForestRegressor(n_estimators=1000, max_depth=8, max_features='sqrt')\n",
    "            elif 'BR' in n:\n",
    "                self.models[n] = BayesianRidge(alpha_1=0.5)\n",
    "            elif 'gbm' in n:\n",
    "                self.models[n] = GradientBoostingRegressor(n_estimators=1000, max_features=0.95,\n",
    "                                                           learning_rate=0.005, max_depth=4)\n",
    "            elif 'lASSO' in n:\n",
    "                self.models[n] = Lasso(alpha=5)\n",
    "\n",
    "   \n",
    "    def __init__(self, model_features):\n",
    "        self.model_names = ['xgb_manifold', 'xgb_grp_bin', 'xgb_pca', 'xgb_mca', 'xgb_bin', 'xgb_trans', 'xgb_pca_bin',\n",
    "                            'xgb_mca_bin', 'xgb_all', 'xgb_grp_trans',\n",
    "                            \n",
    "                            'gbm_manifold', 'gbm_pca', 'gbm_mca', 'gbm_bin', 'gbm_trans', 'gbm_mca_bin', \n",
    "                            'gbm_all', 'gbm_grp_bin', 'gbm_grp_trans'\n",
    "                           \n",
    "                            'BR_manifold', 'BR_bin', 'BR_trans', 'BR_pca_bin', 'BR_mca_bin',\n",
    "                           \n",
    "                            'LASSO_bin', 'LASSO_trans', 'LASSO_pca_bin', 'LASSO_mca_bin', 'LASSO_all', \n",
    "                            'lASSO_grp_bin',\n",
    "                             \n",
    "                            'RF_grp_bin', 'RF_pca', 'RF_mca', 'RF_trans', 'RF_grp_trans',\n",
    "                           \n",
    "                            'xgb_OR_bin', 'xgb_OR_trans', 'xgb_OR_mca_bin', 'gbm_OR_manifold', 'gbm_OR_pca', \n",
    "                            'gbm_OR_bin', 'gbm_OR_trans', 'gbm_OR_mca_bin', 'BR_OR_bin', 'BR_OR_trans', \n",
    "                            'BR_OR_mca_bin', 'LASSO_OR_bin', 'LASSO_OR_pca_bin', 'RF_OR_grp_bin', 'RF_OR_trans']\n",
    "        \n",
    "        self.features = {}\n",
    "        for n in self.model_names:\n",
    "            for f in model_features:\n",
    "                if f in n:\n",
    "                    self.features[n] = model_features[f]\n",
    "                    \n",
    "        self.models = None\n",
    "        self._create_models(self.model_names)\n",
    "        \n",
    "\n",
    "    def fit_one_fold(self, data):\n",
    "        self._create_models(self.model_names)\n",
    "        \n",
    "        for model_name in self.models.keys():\n",
    "            print model_name,\n",
    "            if 'OR' in model_name:\n",
    "                remove_outliers = IsolationForest(contamination=0.05)\n",
    "                remove_outliers = remove_outliers.fit(data[self.features[model_name]].values)\n",
    "                outliers = remove_outliers.predict(data[self.features[model_name]].values)\n",
    "                in_data = data.loc[outliers==1]\n",
    "            else:\n",
    "                in_data = data\n",
    "                \n",
    "            self.models[model_name] = self.models[model_name].fit(in_data[self.features[model_name]].values, \n",
    "                                                                  in_data.y.values)\n",
    "            \n",
    "            \n",
    "    def predict(self, data):\n",
    "        for model_name in self.models.keys():\n",
    "            data[model_name] = self.models[model_name].predict(data[self.features[model_name]].values)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def fit_predict(self, data, kfold):\n",
    "        tmp = pd.DataFrame()\n",
    "        i = 0\n",
    "        for itrain, itest in kfold.values():\n",
    "            print 'Training fold %d: ' % i,\n",
    "            i += 1\n",
    "            self.fit_one_fold(data.iloc[itrain])\n",
    "            x = self.predict(data.iloc[itest])\n",
    "            tmp = tmp.append(x)\n",
    "            print \n",
    "            \n",
    "        self.fit_one_fold(data)\n",
    "        print '\\n--------------------------------------------------'\n",
    "        return tmp\n",
    "    \n",
    "    def fit_predict_data(self, data, kfold):\n",
    "        tmp = pd.DataFrame()\n",
    "        i = 0\n",
    "        for itrain, itest in kfold.values():\n",
    "            print 'Training fold %d: ' % i,\n",
    "            i += 1\n",
    "            self.fit_one_fold(data.iloc[itrain])\n",
    "            x = self.predict(data.iloc[itest])\n",
    "            tmp = tmp.append(x)\n",
    "            print \n",
    "            \n",
    "        print '\\n--------------------------------------------------'\n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combinig model that take the output of the class above and combine it \n",
    "# using two algorithms Random Forest regressor and Bayesian Ridge regression\n",
    "\n",
    "class combine_model:\n",
    "    def __init__(self,comb_features, model_features):\n",
    "        self.stacked_model = None\n",
    "        self.features = model_features\n",
    "        self.comb_features = comb_features\n",
    "        \n",
    "        self.model_RF = RandomForestRegressor(n_estimators=300, max_depth=8, min_samples_leaf=20, oob_score=True,\n",
    "                                  min_samples_split=20, max_features=0.25)\n",
    "    \n",
    "        self.model_BR = BayesianRidge(alpha_1=0.5, alpha_2=0.001, lambda_1=0.001, lambda_2=0.5, normalize=True)\n",
    "        \n",
    "        \n",
    "    def predict(self, data):\n",
    "        data = self.stacked_model.predict(data)\n",
    "        data['y'] = 0.9 * self.model_RF.predict(data[self.comb_features +\n",
    "                                                    self.stacked_model.models.keys()].values) + \\\n",
    "                         0.1 * self.model_BR.predict(data[self.comb_features + \n",
    "                                                       self.stacked_model.models.keys()].values)\n",
    "       \n",
    "        return data\n",
    "    \n",
    "    def fit(self, data):\n",
    "        skf = KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "        i = 0\n",
    "        kfold = {}\n",
    "        for itrain, itest in skf.split(np.zeros(len(data))):\n",
    "            kfold[i] = (itrain, itest)\n",
    "            i += 1\n",
    "            \n",
    "        self.stacked_model = stacked_model(self.features)\n",
    "        stack_output = self.stacked_model.fit_predict(data, kfold)\n",
    "        \n",
    "        self.model_RF = self.model_RF.fit(stack_output[self.comb_features + self.stacked_model.models.keys()].values,\n",
    "                                          stack_output.y.values)\n",
    "        self.model_BR = self.model_BR.fit(stack_output[self.comb_features + self.stacked_model.models.keys()].values,\n",
    "                                   stack_output.y.values)\n",
    "        return self\n",
    "    \n",
    "    def prepare_data(self, data):\n",
    "        skf = KFold(n_splits=5, shuffle=True, random_state=7609)\n",
    "        i = 0\n",
    "        kfold = {}\n",
    "        for itrain, itest in skf.split(np.zeros(len(data))):\n",
    "            kfold[i] = (itrain, itest)\n",
    "            i += 1\n",
    "            \n",
    "        self.stacked_model = stacked_model(self.features)\n",
    "        stack_output = self.stacked_model.fit_predict_data(data, kfold)\n",
    "        \n",
    "        return stack_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining features to create multiple sets of features to be use as \n",
    "# an input to different machine learning models\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=9856)\n",
    "Y_pred = []\n",
    "Y_true = []\n",
    "scores = []\n",
    "\n",
    "model_features = {}\n",
    "model_features['all'] = bin_features + cat_bin_features  + tsne_features + isomap_features + \\\n",
    "                        trans_features  + grp_features + mca_features + pca_all_features + pca_bin_features + \\\n",
    "                        mca_bin_features\n",
    "        \n",
    "model_features['manifold'] = bin_features + cat_bin_features  + tsne_features + isomap_features\n",
    "model_features['grp_trans'] = trans_features  + grp_features\n",
    "model_features['grp_bin'] = bin_features + cat_bin_features  + grp_features\n",
    "model_features['pca'] = trans_features  + pca_all_features\n",
    "model_features['mca'] = trans_features  + mca_features\n",
    "model_features['bin'] = bin_features + cat_bin_features\n",
    "model_features['trans'] = trans_features\n",
    "model_features['pca_bin'] = bin_features + cat_bin_features + pca_bin_features\n",
    "model_features['mca_bin'] = bin_features + cat_bin_features + mca_bin_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0:  xgb_pca_bin gbm_manifold lASSO_grp_bin xgb_mca gbm_trans gbm_OR_trans gbm_grp_transBR_manifold gbm_all RF_grp_bin gbm_OR_manifold BR_bin xgb_mca_bin gbm_OR_bin xgb_OR_trans gbm_mca RF_trans gbm_bin xgb_grp_bin BR_mca_bin xgb_bin BR_trans gbm_mca_bin BR_OR_bin xgb_OR_bin RF_OR_grp_bin RF_mca xgb_trans RF_grp_trans RF_OR_trans xgb_pca BR_OR_trans gbm_pca gbm_OR_pca gbm_grp_bin xgb_manifold xgb_all xgb_OR_mca_bin BR_OR_mca_bin gbm_OR_mca_bin xgb_grp_trans RF_pca BR_pca_bin\n",
      "Training fold 1:  xgb_pca_bin gbm_manifold lASSO_grp_bin xgb_mca gbm_trans gbm_OR_trans gbm_grp_transBR_manifold gbm_all RF_grp_bin gbm_OR_manifold BR_bin xgb_mca_bin gbm_OR_bin xgb_OR_trans gbm_mca RF_trans gbm_bin xgb_grp_bin BR_mca_bin xgb_bin BR_trans gbm_mca_bin BR_OR_bin xgb_OR_bin RF_OR_grp_bin RF_mca xgb_trans RF_grp_trans RF_OR_trans xgb_pca BR_OR_trans gbm_pca gbm_OR_pca gbm_grp_bin xgb_manifold xgb_all xgb_OR_mca_bin BR_OR_mca_bin gbm_OR_mca_bin xgb_grp_trans RF_pca BR_pca_bin\n",
      "Training fold 2:  xgb_pca_bin gbm_manifold lASSO_grp_bin xgb_mca gbm_trans gbm_OR_trans gbm_grp_transBR_manifold gbm_all RF_grp_bin gbm_OR_manifold BR_bin xgb_mca_bin gbm_OR_bin xgb_OR_trans gbm_mca RF_trans gbm_bin xgb_grp_bin BR_mca_bin xgb_bin BR_trans gbm_mca_bin BR_OR_bin xgb_OR_bin RF_OR_grp_bin RF_mca xgb_trans RF_grp_trans RF_OR_trans xgb_pca BR_OR_trans gbm_pca gbm_OR_pca gbm_grp_bin xgb_manifold xgb_all xgb_OR_mca_bin BR_OR_mca_bin gbm_OR_mca_bin xgb_grp_trans RF_pca BR_pca_bin\n",
      "Training fold 3:  xgb_pca_bin gbm_manifold lASSO_grp_bin xgb_mca gbm_trans gbm_OR_trans gbm_grp_transBR_manifold gbm_all RF_grp_bin gbm_OR_manifold BR_bin xgb_mca_bin gbm_OR_bin xgb_OR_trans gbm_mca RF_trans gbm_bin xgb_grp_bin BR_mca_bin xgb_bin BR_trans gbm_mca_bin BR_OR_bin xgb_OR_bin RF_OR_grp_bin RF_mca xgb_trans RF_grp_trans RF_OR_trans xgb_pca BR_OR_trans gbm_pca gbm_OR_pca gbm_grp_bin xgb_manifold xgb_all xgb_OR_mca_bin BR_OR_mca_bin gbm_OR_mca_bin xgb_grp_trans RF_pca BR_pca_bin\n",
      "Training fold 4:  xgb_pca_bin gbm_manifold lASSO_grp_bin xgb_mca gbm_trans gbm_OR_trans gbm_grp_transBR_manifold gbm_all RF_grp_bin gbm_OR_manifold BR_bin xgb_mca_bin gbm_OR_bin xgb_OR_trans gbm_mca RF_trans gbm_bin xgb_grp_bin BR_mca_bin xgb_bin BR_trans gbm_mca_bin BR_OR_bin xgb_OR_bin RF_OR_grp_bin RF_mca xgb_trans RF_grp_trans RF_OR_trans xgb_pca BR_OR_trans gbm_pca gbm_OR_pca gbm_grp_bin xgb_manifold xgb_all xgb_OR_mca_bin BR_OR_mca_bin gbm_OR_mca_bin xgb_grp_trans RF_pca BR_pca_bin\n",
      "xgb_pca_bin gbm_manifold lASSO_grp_bin xgb_mca gbm_trans gbm_OR_trans gbm_grp_transBR_manifold gbm_all RF_grp_bin gbm_OR_manifold BR_bin xgb_mca_bin gbm_OR_bin xgb_OR_trans gbm_mca RF_trans gbm_bin xgb_grp_bin BR_mca_bin xgb_bin BR_trans gbm_mca_bin BR_OR_bin xgb_OR_bin RF_OR_grp_bin RF_mca xgb_trans RF_grp_trans RF_OR_trans xgb_pca BR_OR_trans gbm_pca gbm_OR_pca gbm_grp_bin xgb_manifold xgb_all xgb_OR_mca_bin BR_OR_mca_bin gbm_OR_mca_bin xgb_grp_trans RF_pca BR_pca_bin \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the staked model and make prediction\n",
    "\n",
    "comb_features = bin_features + trans_features\n",
    "    \n",
    "model = combine_model(comb_features, model_features)\n",
    "\n",
    "model = model.fit(data)\n",
    "\n",
    "test_data = model.predict(test_data)\n",
    "\n",
    "test_data[['ID', 'y']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
